diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/slaves.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/slaves.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/slaves.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/slaves.sh	2021-02-20 09:59:26.190739000 -0800
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -80,19 +82,19 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
-
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
 
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `echo "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
   if [ -n "${SPARK_SSH_FOREGROUND}" ]; then
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /"
   else
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /" &
   fi
   if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/spark-daemon.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/spark-daemon.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/spark-daemon.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/spark-daemon.sh	2021-02-20 09:59:26.194738000 -0800
@@ -167,7 +167,8 @@ run_command() {
 
   if [ "$SPARK_MASTER" != "" ]; then
     echo rsync from "$SPARK_MASTER"
-    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "${SPARK_HOME}"
+    RSH_CMD=${SPARK_SSH_CMD:-ssh}
+    rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "${SPARK_HOME}"
   fi
 
   spark_rotate_log "$log"
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/spark-daemons.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/spark-daemons.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/spark-daemons.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/spark-daemons.sh	2021-02-20 09:59:26.199726000 -0800
@@ -31,6 +31,24 @@ if [ -z "${SPARK_HOME}" ]; then
   export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
-exec "${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/spark-daemon.sh" "$@"
+exec "${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/spark-daemon.sh" "$@"
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slave.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slave.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slave.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slave.sh	2021-02-20 09:59:26.203737000 -0800
@@ -50,6 +50,24 @@ if [[ $# -lt 1 ]] || [[ "$@" = *--help ]
   exit 1
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
@@ -79,8 +97,14 @@ function start_instance {
   fi
   WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))
 
-  "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \
-     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  if [ "${SPARK_CONF_DIR}X" != "X" ]
+  then
+    "${SPARK_HOME}/sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start $CLASS $WORKER_NUM \
+	--webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  else
+    "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \
+	--webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  fi
 }
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slave.sh.orig spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slave.sh.orig
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slave.sh.orig	1969-12-31 16:00:00.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slave.sh.orig	2021-02-20 09:59:26.206768000 -0800
@@ -0,0 +1,92 @@
+#!/usr/bin/env bash
+
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Starts a slave on the machine this script is executed on.
+#
+# Environment Variables
+#
+#   SPARK_WORKER_INSTANCES  The number of worker instances to run on this
+#                           slave.  Default is 1. Note it has been deprecate since Spark 3.0.
+#   SPARK_WORKER_PORT       The base port number for the first worker. If set,
+#                           subsequent workers will increment this number.  If
+#                           unset, Spark will find a valid port number, but
+#                           with no guarantee of a predictable pattern.
+#   SPARK_WORKER_WEBUI_PORT The base port for the web interface of the first
+#                           worker.  Subsequent workers will increment this
+#                           number.  Default is 8081.
+
+if [ -z "${SPARK_HOME}" ]; then
+  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
+fi
+
+# NOTE: This exact class name is matched downstream by SparkSubmit.
+# Any changes need to be reflected there.
+CLASS="org.apache.spark.deploy.worker.Worker"
+
+if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then
+  echo "Usage: ./sbin/start-slave.sh <master> [options]"
+  pattern="Usage:"
+  pattern+="\|Using Spark's default log4j profile:"
+  pattern+="\|Started daemon with process name"
+  pattern+="\|Registered signal handler for"
+
+  "${SPARK_HOME}"/bin/spark-class $CLASS --help 2>&1 | grep -v "$pattern" 1>&2
+  exit 1
+fi
+
+. "${SPARK_HOME}/sbin/spark-config.sh"
+
+. "${SPARK_HOME}/bin/load-spark-env.sh"
+
+# First argument should be the master; we need to store it aside because we may
+# need to insert arguments between it and the other arguments
+MASTER=$1
+shift
+
+# Determine desired worker port
+if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then
+  SPARK_WORKER_WEBUI_PORT=8081
+fi
+
+# Start up the appropriate number of workers on this machine.
+# quick local function to start a worker
+function start_instance {
+  WORKER_NUM=$1
+  shift
+
+  if [ "$SPARK_WORKER_PORT" = "" ]; then
+    PORT_FLAG=
+    PORT_NUM=
+  else
+    PORT_FLAG="--port"
+    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))
+  fi
+  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))
+
+  "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \
+     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+}
+
+if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
+  start_instance 1 "$@"
+else
+  for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
+    start_instance $(( 1 + $i )) "$@"
+  done
+fi
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slaves.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slaves.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/start-slaves.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/start-slaves.sh	2021-02-20 09:59:26.210750000 -0800
@@ -43,4 +43,4 @@ if [ "$SPARK_MASTER_HOST" = "" ]; then
 fi
 
 # Launch the slaves
-"${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
+"${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/start-slave.sh" --config $SPARK_CONF_DIR "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/stop-slave.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/stop-slave.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/stop-slave.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/stop-slave.sh	2021-02-20 09:59:26.214743000 -0800
@@ -31,6 +31,23 @@ if [ -z "${SPARK_HOME}" ]; then
   export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir="$1"
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR="$conf_dir"
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
diff -pruN spark-3.0.2-bin-hadoop2.7-orig/sbin/stop-slaves.sh spark-3.0.2-bin-hadoop2.7-alternate/sbin/stop-slaves.sh
--- spark-3.0.2-bin-hadoop2.7-orig/sbin/stop-slaves.sh	2021-02-15 21:21:51.000000000 -0800
+++ spark-3.0.2-bin-hadoop2.7-alternate/sbin/stop-slaves.sh	2021-02-20 09:59:26.219749000 -0800
@@ -25,4 +25,4 @@ fi
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
 
-"${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin"/stop-slave.sh
+"${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin"/stop-slave.sh --config $SPARK_CONF_DIR
